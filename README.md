# Project Skeleton - Structured ML/DL Project Template

Structured project template for Machine Learning/Deep Learning with PyTorch. Implements best practices for organization, reproducibility, and collaboration.

**Forked from:** `iurada/project-skeleton:main`

---

## ğŸ“ Project Structure

```
polito-aml-project_skeleton/
â”œâ”€â”€ checkpoints/                  # ğŸ’¾ MODEL CHECKPOINTS (created during training)
â”‚   â”œâ”€â”€ .gitkeep                  # Keeps folder in git
â”‚   â”œâ”€â”€ best_model.pth            # Best model saved automatically (gitignored)
â”‚   â””â”€â”€ checkpoint_epoch_N.pth    # Periodic checkpoints (gitignored)
â”‚
â”œâ”€â”€ data/                         # ğŸ“ DATASET FILES (gitignored - download separately)
â”‚   â”œâ”€â”€ .gitkeep
â”‚   â”œâ”€â”€ training_set/             # Training images (cats/, dogs/)
â”‚   â””â”€â”€ test_set/                 # Test images (cats/, dogs/)
|
â”œâ”€â”€ dataset/                      # ğŸ“¦ DATASET MODULE
â”‚   â”œâ”€â”€ __init__.py               # Exports: CustomImageDataset, create_annotations_csv
â”‚   â””â”€â”€ custom_dataset.py         # PyTorch Dataset class for data loading from CSV
â”‚
â”œâ”€â”€ models/                       # ğŸ§  MODELS MODULE
â”‚   â”œâ”€â”€ __init__.py               # Exports: create_name_model
â”‚   â””â”€â”€ vgg_finetuning.py         # Model architectures
â”‚
â”œâ”€â”€ utils/                        # ğŸ› ï¸ UTILITIES MODULE
â”‚   â”œâ”€â”€ __init__.py               # Exports: transforms, visualization, metrics functions
â”‚   â”œâ”€â”€ download_dataset.py       # DATASET DOWNLOADER (downloads dataset, e.g., from Kaggle)
â”‚   â”œâ”€â”€ transforms.py             # Data augmentation and preprocessing (train/val/test)
â”‚   â”œâ”€â”€ visualization.py          # Plotting and visualizations (denormalize, plot curves)
â”‚   â””â”€â”€ metrics.py                # Metrics computation and dataset statistics
â”‚
â”œâ”€â”€ train.py                      # ğŸš‚ TRAINING SCRIPT (main training loop with CLI)
â”œâ”€â”€ eval.py                       # ğŸ“Š EVALUATION SCRIPT (test set evaluation with CLI)
â”œâ”€â”€ config.py                     # âš™ï¸ CONFIGURATION (hyperparameters and central settings)
â”‚
â”œâ”€â”€ colab_training.ipynb          # ğŸ““ GOOGLE COLAB NOTEBOOK (training on Colab)
â”œâ”€â”€ requirements.txt              # ğŸ“‹ PYTHON DEPENDENCIES (pip install -r requirements.txt)
â”œâ”€â”€ .gitignore                    # ğŸš« GIT IGNORE (data/, checkpoints/*.pth, *.csv, wandb/)
â”‚
â””â”€â”€ README.md
```

---

## ğŸ¯ Implemented Best Practices

âœ… **Modularity**: Code split into reusable modules

âœ… **CLI Interface**: Argparse for all scripts

* **What is it?** Command-Line Interface allows running scripts from the terminal by passing parameters as options (e.g., `--epochs 10 --lr 0.001`)
* **Benefits:** No need to modify the code for each experiment; all parameters are configurable from the command line
* **Implementation:** Uses Pythonâ€™s `argparse` to define all available arguments (data_dir, epochs, batch_size, learning rate, etc.)

âœ… **Reproducibility**: requirements.txt + config.py

* **config.py** defines default values and project constants
* **CLI arguments** allow overriding defaults without code changes
* The two approaches are complementary: config.py is the â€œcontrol center,â€ the CLI provides flexibility for experiments

âœ… **Checkpoint Management**: Auto-save best model

âœ… **Logging**: Wandb integration

âœ… **Documentation**: Docstrings + complete README

âœ… **Git-friendly**: Proper .gitignore

âœ… **Data Augmentation**: Only on train, not on val/test

âœ… **Separation of Concerns**: train.py vs eval.py

---

## ğŸ” For AI Assistants

**This project follows a standard modular structure:**

1. **Dataset Module** (`dataset/`): Data loading management
2. **Models Module** (`models/`): Architectures and model creation
3. **Utils Module** (`utils/`): Transforms, visualization, metrics
4. **Training Script** (`train.py`): Main training loop with CLI
5. **Eval Script** (`eval.py`): Test set evaluation
6. **Config** (`config.py`): Centralized configuration

**Key Points:**

* Each module has an `__init__.py` with explicit exports
* CLI scripts use argparse
* Training (train/validate/test functions)
* Automatic checkpoint management
* Optional but complete Wandb integration
* Transforms: AUGMENTATION only on train!

**When to suggest modifications:**

* Add new models â†’ `models/new_model.py`
* New metrics â†’ `utils/metrics.py`
* New datasets â†’ `dataset/new_dataset.py`
* Training modifications â†’ `train.py` (keep CLI style)

---

## ğŸ¤ Contributing

To adapt this skeleton to your project:

1. **Dataset**: Modify `dataset/custom_dataset.py` for your format
2. **Model**: Add your architecture in `models/`
3. **Config**: Update `config.py` with your parameters
4. **Training**: Modify `train.py` if needed (keep CLI)
5. **Update README**: Document your changes

---

## ğŸš« Git Ignore (`.gitignore`)

**What it ignores:**

* `data/` â€“ Dataset (too large, downloaded separately)
* `checkpoints/*.pth` â€“ Model checkpoints (too large)
* `*.csv` â€“ Annotation files (generated automatically)
* `wandb/` â€“ Wandb logs (synced to cloud)
* `__pycache__/` â€“ Python cache
* `.DS_Store` â€“ macOS files

**What it tracks:**

* Source code (`.py`)
* Configurations
* README and docs
* `.gitkeep` for empty folders

---

## ğŸ”„ Typical Workflow

### 1. Initial Setup

```bash
git clone <repo-url>
cd polito-aml-project_skeleton
pip install -r requirements.txt
python download_dataset.py
```

### 2. Training

```bash
# Feature extraction (base frozen)
python train.py --data_dir ./data --epochs 10 --freeze_base --use_wandb

# Full fine-tuning (everything trainable)
python train.py --data_dir ./data --epochs 10 --use_wandb
```

### 3. Evaluation

```bash
python eval.py --checkpoint ./checkpoints/best_model.pth --data_dir ./data
```

### 4. Experiments

```bash
# Experiment with different LR
python train.py --lr 0.001 --batch_size 64 --use_wandb

# All experiments tracked on Wandb!
```

---

## ğŸ“¢ Release Information

**ğŸ“… Last update:** November 2025
**ğŸ·ï¸ Version:** v1.0.0 â€” First stable release

*For details on changes and fixes, see the changelog in the repository.*
